\documentclass[journal]{IEEEtran}
\usepackage{graphicx}
\usepackage{pdfpages}
%\usepackage{authblk} % to allow affiliation [N] in the author list
\usepackage{textcomp} % proper tilde
\usepackage{multirow}
\usepackage{cite}

\begin{document}

\title{High Performance Detector Software\\
for PERCIVAL Detector}

\author{~Q. Gu}

% Page Header
\markboth{Diamond Light Source Summer Internship Program Report}%
{Shell \MakeLowercase{\textit{et al.}}: Feasibility Study of PERCIVAL Data
Acquisition Backend Architecture}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

\IEEEPARstart{T}{he} PERCIVAL\cite{wunderer2014percival} (Pixelated, Energy Resolving CMOS Imager, Versatile And Large) detector is a new generation X-ray detector aiming to achieve real-time data capturing, pre-processing and storing. This report describes how a highly optimised C++ library had been designed to pre-process Percival output. \\ 
Percival is capable of outputing \texttildelow 13M pixels (3717 $\times$ 3528 pixels) at 120fps (frames per second). Each pixel is stored as a 16-bit integer packed from 3 separate integer fields, knowns as the Fine, Coarse and Gain bits. The total data rate is \texttildelow 6GB/s. The meaning of the packed integer is not at all obvious to users without being converted to numbers proportional to intensity. Preprocessing is there required to do this conversion.

\begin{table}[h]
\centering
\label{bit_arrangement}
\begin{tabular}{r l c c c r l c c c c c c  r l r}
\hline
\multicolumn{1}{|c|}{} & 
\multicolumn{5}{|c|}{Coarse (C)} & 
\multicolumn{8}{|c|}{Fine (F)} &
\multicolumn{2}{|c|}{Gain} \\
\hline
 15 & 14 & & & & 11 & 10 & & & & & & & 2 & 1 & 0 \\
\end{tabular}
\caption{Bit arrangement.}
\end{table}

The processing backend consists of a cluster of eight nodes (numbered 77, 78, 83~88) each receiving a full frame of data. There are existing networks that evenly distribute the output to each cluster node. The pre-processing library therefore needs to process at \texttildelow 750MB/s per node. As 5 physical threads need to be set aside for other applications running on the server (receiving frames and writing to disc), the computing power is not fully accessible. 

\begin{table}[h]
\centering
\label{servers}
\begin{tabular}{l | c c}
\hline
Micro-architecture & Intel SandyBridge & Intel IvyBridge \\
\hline
Processor Number& 	E5-2670 0	& E5-2670 v2 \\
Number of machines&	2		& 6	\\
L1D cache	&	32K		& 32K \\
L2 cache (shared)&	256K		& 256K \\
L3 cache (LLC)	&	20MB (shared)	& 25MB (shared) \\
Core per processor  &	8		& 10	\\
Number of processors  &	2		& 2	\\
Memory bandwidth    &51.2GB/s       & \\
\hline
Other supports  & 
\multicolumn{2}{|c}{AVX, HyperThreading, TurboBoost}  	\\
\hline
\end{tabular}
\caption{Specification of Server Nodes}
\end{table}


From the pre-processing library's perpective, upstream functions stream detector output into local memory buffers from which the library takes its input. Downstream applications then write data from the buffer to disc after the library fills the memory buffer with processed data. \\ \\
Processing steps required of this library are as follows,
\begin{itemize}
 \item \textbf{Step 1: ADC coarse, ADC Fine, and Gain decoding} Extract bits from packed 16-bit integer and convert digitized figures to floating point numbers (Calibration).
	\begin{equation}
		ADU(i,j) = \frac{C - O_F}{G_F} - \frac{F - O_F}{G_F}
	\end{equation}
 \item \textbf{Step 2: Gain Multiplication} Multiply previous results by a factor depending on location.
	\begin{equation}
		I_{sample}(i,j) = ADU(i,j) \times K(G_{sample}(i,j))
	\end{equation}
where K is a constant depending on pixel location and value of GainBits.	
 \item \textbf{Step 3: CDS subtraction} Subtract reset frame that has gone through previous two steps if the corresponding pixels in sample frame has gain bit 0b00.
	\begin{equation}	
		I_0(i,j) = I_{sample}(i,j) - I_{reset}(i,j) \times \delta(G_{sample}=0b00)
	\end{equation}
 \item \textbf{Step 4: Dark Image Subtraction} Subtract a constant frame from the result of previous step.
	\begin{equation}
		I(i,j) = I_0(i,j) - Dark(i,j)
	\end{equation}
\end{itemize}

$Gc, Gf, Oc, Of, K, Dark$ are all constant arrays intrinsic to the detector and Analogue to Digital Converters (ADCs), supplied before data is gathered. It was required that Step 1 could be run separately. Two functions are written, one for Step 1 only and the other for a combined algorithm using Step 1 to Step 4. As further linear corrections might still be needed, the function and calibration data container are organiseded such that further computation steps can be added easily.\\

%table of sizes of arrays.
\begin{table}[h]

\label{array_dims}
\centering
\begin{tabular}{l c c c}
\hline
Array 							& Width(pixels)	 & Datatype  &Size\\
\hline\hline
Sample/Reset Frame       				& 3528 		& uint16     &25MB\\
\hline
Output from step 4					& 3528		& float32    &50MB\\
\hline
Dark(unaligned)					 	& 3528		& float32     &50MB\\
\hline
Dark(aligned)				 		& 4032 		& float32     &57MB\\
\hline
$G_C/G_F/O_C/O_F$ unaligned 				& 7   		 & float32    &50MB\\
\hline
$G_C/G_F/O_C/O_F$ aligned 				& 8    		& float32     &57MB\\
\hline
$K_{00}/K_{01}/K_{10}/K_{11}$ unaligned 	& 3528 & float32     & 116KB\\
\hline
$K_{00}/K_{01}/K_{10}/K_{11}$ aligned 		& 4032 & float32     & 102KB\\
\hline

\end{tabular}
\caption{Array dimensions}
\end{table}

A python version of processing step 1 was implemented as a prototype. This code was then used to assit and check development in C++. The C++ library was designed with an automatic test suite as scaffolding (Test-Driven Development). Statistical profiling tool OProfile was used to identify the bottleneck limiting library performance. Initial optimisation effort was spent primarily on making room for compiler optimisation using GCC compiler. In a later stage, the library also incorporated more advanced tools including Intel Threading Building Blocks (TBB) and Intel Advanced Vector Extensions (AVX). The main approaches adopted will be discussed in section \ref{sec:perf}


\section{Test Driven Development (TDD)} %page2

The development process was guided by a set of tests which act as a specifications to check the code against. Each function is written in order to pass the tests. The tests are run each time some part of the code has been modified. The tests utilized BOOST Unit Test Framework (UTF). These tests follows the following guidelines.
\begin{itemize}
 \item \textbf{Range Tests:} Test if array dimensions are consistent with table.
 \item \textbf{Computation Tests:} Test if the result returned from the function agrees with manually calculated results within $0.01\%$.	
\end{itemize}

\section{Organisation}	%page2
The library builds upon basic constructs including the memory buffer, the calibration data container, the data file location container, exception classes and functions that check data validity. More advanced features important to optimisation such as the parallelised algorithms and algorithms assisted by Intel Advanced Vector Extensions are also supplied in separate header files. In order to accommadate these extended features, the basic constructs have been adapted slightly. Particularly, all processing functions are implemented using function objects with a thin function wrapper. A few Python scripts have also been included to facilitate processing profiling and timing results.

\subsection{Basic Constructs}
Firstly, the most fundamental construct is a templated structure, percival$\_$frame, containing information about a frame, including width, height and a pointer to memory buffer containing data in the frame, organised as contiguous linear array in memory. This structure underlies all functions that use arrays. Another flavour of of this structure is capable of creating, deleting and aligning memory to 4K memory page. However, this structure should only be used in testing environment as its memory management is not safe to use without knowing its inner working. Secondly, calibration data come in the form of percival$\_$frame and are grouped into one structure percival$\_$calib$\_$params that can be passed to functions when calibration arrays are needed. These arrays are preloaded and pre-processing before processing starts. Finding the inverses of $G_C$ and $G_F$ is done in preloading. This pre-loading can be customised using user-defined functions. Thirdly, non-parallelised processing algorithm for each step and ADC decode are supplied. \\
In conjunction with these fundamental building blocks, other helper tools including exception classes, functions that check data validity (range, NULL pointer), an HDF5 loader, an HDF5 writer and a structure containing locations of calibration files were also supplied. However the HDF5 loader and writer are not extensively tested and should not be used in release version. \\

\subsection{Parallelised Algorithm}
A parallelised version of algorithm containing all four steps are written. They are suffixed with $\_pf$. Other processing steps are also supplied and are based on the same function objects as used basic, non-parallelised functions.

\subsection{AVX algorithm}
A function object written in AVX intrinsics is available for combined processing algorithm encompassing step 1 to 4. They have to be compiled with AVX instruction set enabled and run on an AVX-enabled operating systems and processors. More descriptions about AVX and intrinsics are in section \ref{Vec}.

\subsection{Tools}
Tools for generating test calibration parameters, for processing OProfile output and compute useful metrics, for iteratively test library parameters, and a driver for using the library (C++ main function) have also been included.

\section{Performance Analysis and Tuning} %page3&4
\label{sec:perf}
The critical requirement on this library is processing time, as opposed to other aspects such as power consumption and memory usage. Therefore, the key indicator of performance is the bandwidth, defined as amount of data processed per second, counting both the sample and reset frames. Processing one sample and one reset frame in one second corresponds to a processing rate of 52MB/s. The general approach to performance tuning starts with profiling, which gives detailed report on how much time/memory were spent in each function/line of code/file. These key indicators reflect the bottleneck of the target program and are very important, though not necessarily accurate, guide on how best to optimise the library. Typically, target codes are either memory-bound or core-bound (bounded by computation). There are standard approaches correspond to either category. These tuning is often specific to hardware, particularly on the micro-architecture of the system.
\subsection{Identify the Hotspots}
\subsubsection{Profiling Tools}
In our study, we have considered both instrumentation and statistical profiling tools. Instrumentation, with gprof as an example, requires compilers to add extra code to user code, thus incurring more errors in measurement. On the other hand, statistical profiling uses hardware counters to count system interrupts when certain events happen on the hardware level. This, though inevitably slows down the processor, is known to generate more accurate results. We thus used OProfile, a statistical profiling tool, to study our system. As hardware counters are used to profile, the OProfile events are hardware-dependent. However, similar events can be found on different platforms, tough often with different names. Some of the OProfile event count are ready to use in the state they are generated, but some require processing before they can be truly useful. The Intel Optimisation Manual gives an extensive guide on how to use these hardware events. The key performance metrics useful to our tests are listed in table \ref{Operfmetrics}. These metrics are all ratios of duration-dependent quantities and are independent of duration of profiling. 

\begin{table*}[ht]
\centering
\begin{tabular}{l l l c c}
\hline
Type 					& Metric   			& Explanation  					& min sampling rate	\\
\hline\hline
\multirow{3}{*}{Time}
& CLK$\_$CYCLE$\_$UNHALTED 						& No of clock cycles 				& 10000			& event\\
& INST$\_$retired  							& instructions counted in the retirement unit 	& 6000			& event\\
& CLK$\_$CYCLE$\_$UNHALTED / INST$\_$retired 				& cycle per instruction				& -			& metric\\

\hline

\multirow{3}{*}{Cache}
& LLC$\_$MISSES								& llc misses					& 6000			& event\\
& L2$\_$lines$\_$in							& Incomming cache lines to L2			& 100000		& event\\
& LLC$\_$misses / INST$\_$retired 					& llc misses per instruction			& -			& metric\\
& L2$\_$lines$\_$in / INST$\_$retired 					& L2 misses per instruction 			& -			& metric\\

\hline
\end{tabular}
\caption{Key OProfile events and Performance metrics.}\label{Operfmetrics}
\end{table*}

\subsubsection{Interpreting OProfile Results}
OProfile sets a constant upper limit, the sampling rate, on the hardware counters. The counters count and accumulate system events and generate an interrupt once the upper limit is reached. The number of these interrupts, termed number of samples, is collected and reported as OProfile output. The actual number of events is approximately the product of sampling rate and number of samples collected. Too high a sampling rate reduces the resolution of profiling. Too small a sampling rate results in buffer overflow, which could be solved by setting a higher buffer size or raising the sampling rate. In addition, it has been experimentally observed that counting too many events simultaneously affects profile output in an undeterministic manner. As a guide, count only less than 6 events each time. \\

OProfile is also able to give line-by-line profiling results along side the source code. This feature helps to get a more fine-grained idea of where the hotspot is. However, the indicated location often lags the real location by a few instructions, also known as skids. These cannot be avoided using current hardware. \\

On a side note, the CLK$\_$CYCLE$\_$UNHALTED event is not suitable as a timer. For machines with TurboBoost enabled, the clock rate changes with overclocking. In addition, CLK$\_$CYCLE$\_$UNHALTED event counts cycles in all CPUs rather than the CPU that spawns the program. The time thus calculated will be ten or more times higher than measured using wall clock.  \\

\subsection{Optimising the Algorithm}
We performed baseline measurements on both single-threaded and multi-threaded codes, which indicated the floating-point computation in equation 1 limits the bandwidth. We thus started on reducing clock cycles spent on computation.
\subsubsection{Reducing Computation Time}
Floating point divisions are inherently much more costly than multiplication and addition. The first step to optimisation lies in converting $G_C$ and $G_F$ to their inverses before actual computation starts. During the processing, division by $G_C$ and $G_F$ is replaced by multiplication of their inverses. These operations are done internally and do not require supplier of calibration data to perform extra computation.\\
Similarly, bitwise operations were also used to replace integer modulus and division.\\

\subsection{Other Optimisation}
GCC compiler offers three levels of optimisation with the -O1, -O2, -O3 command line options. One explicit hint to the compiler is that variables were declared out of a loop to avoid repeated allocation and deallocation of memory. Furthermore, pointer dereferencing and function calls overheads are significant on micro-second scale and are both avoided in loops. Other techniques provided by C++ language was also applied, for example, const keyword is used whenever possible, though the speed up is not as spetacular. 

\subsubsection{Parallelisation}
Intel Threading Building Blocks (Intel TBB) was employed to parallelize the library. Advantages of Intel TBB include an automatic management of task scheduling and load-balancing. Using its automatic scheduler, the number of threads used is determined at run-time and has been optimised to achieve best performance, usually when logical threads match physical threads. An integer, $N$, can also be supplied to the task scheduler at initialisation and this generates $(N-1)$ working threads, with one thread that spawns the task. \\

TBB builds upon C++ object-oriented paradigm and are more user-friendly than conventional pthread. The parallel$\_$for templated function and the pipeline pattern have both been tested with our library. The results are peculiar. The use of Pipeline pattern doubles the bandwidth of the target function. Overhead due to TBB scheduler when Pipeline patter is used is ten times that when parallel-for template function is used. As a result, Pipeline is a better choice when the bandwidth is \texttildelow 600MB/s and parallel-for is better at bandwidth \texttildelow 2GB/s. \\

The parallel$\_$for template function uses tbb::blocked$\_$range object to segment the iteration space. This iterator object splits ranges by recursively halving the interval. Split stops when the current size of the subrange is strictly smaller than the grain size specified. This introduces some subtleties here. Firstly, algorithm based on AVX requires each sub-range to be multiples of 7 which is not trivial when recursive splitting is used. Secondly, the subranges are a mixtures of different sizes as the overall range has factors other than 2 in general. Therefore, in the implementation, splitting is performed on height of frame (0\texttildelow3717) rather than on the entire array (0\texttildelow13M). The subranges are then guaranteed to be multiples of the grain size. The user has to ensure that the grain size is a multiple of seven, otherwise exception will be thrown. Furthermore, when grain size is varied to tune the library, a combined effect of subranges of various sizes will be observed. Therefore too fine a variation in grain size would not yield very meanful results.

\subsubsection{Vectorisation}
\label{Vec}
Intel IvyBirdge and SandyBridge architecture both allow Intel Advanced Vector Extensions (AVX) instruction sets. Intel AVX utilizes 256-bit wide YMM registers to perform Single Instruction Multiple Data (SIMD) on packed floating point numbers. AVX theoretically has twice the computation power, compared to previous Streaming SIMD Extensions (SSE, using 128-bit XMM registers) which is automatically enabled by many compiler optimisers (-O3 option in gcc). \\
There are currently three approach to utilise Intel AVX, assembly code, intrinsics and Intel Integrated Performance Primitives (IPP). One great aspect of IPP is that commonly used multi-media processing functions are written in C functions and implemented using vectorised code. It is more user-friendly when complex computations are concerned. Assembly code, on the other hand, lends much more flexibility with a price of readability. \\

Among the three options, Intel Intrinsics are particularly suitable for our simple computations at hand. Intrinsics are a set of C style functions incorporated into some compilers (including GCC) to facilitate use of new instruction set. The basic data flow involves first packing floating point data into $\_\_$m256 datatype, performing operations using intrinsic functions, and store the data to a memory buffer. The major operations used has been listed in table \ref{ops}. $\_\_$m256 datatype can contain 8 packed single precision float (ps) or 4 packed double precision float (pd). Intrinsics working on these datatypes have $\_$ps and $\_$pd sufficies to distinguish the operand. Bit or logical operations can be done only on packed int32 ($\_\_$mm256i with $\_$epi suffix) and conversion or casting is needed. Value of each packed element remains the same after conversion and Bit arrangement remains the same after casting. These two functions are also supplied. \\

Compared with the operations themselves, loading and storing between memory buffer and $\_\_$m256 datatype present more opportunities for tuning. Accessing memory unaligned with 256-bit boundaries causes minimal improvement of AVX from SSE. In our situation, each access to a sample and reset frame pair is accompanied by 4 accesses to gain lookup tables, 1 accesses to dark image table, and 1/126 accesses to $G_C, O_C, G_F, O_F$. Forcing the sample and reset frames to align causes excessive memory copying and is not practical. However, aligning the rest of the 5 accesses is easy. We therefore insert one zero element for every 7 element in the calibration arrays. Padding significantly improved performance. \\

When AVX is not available, enabling -O3 optimisation using GCC compiler enables auto-vectorisation using SSE instruction set by the compiler. SSE2 is useful in our application where floating point algorithms are concerned. Auto-vectorisation does not always realise the potential of SSE and manually coding vectorised code might be better. \\

\begin{table}[h]
\centering

\begin{tabular}{l l c c c}
\hline
Instruction & Operation & Datatype & Latency & Throughput \\
\hline \hline
\multirow{4}{*}{AVX}
 & mul & ps 	& 5 	& 1 \\
 & add & ps 	& 3 	& 1 \\
 & bit & ps 	& 1 	& 1 \\
 & cmp & ps 	& 3 	& 1 \\
\hline

\multirow{4}{*}{x86}
 & mul & int 	& 3 	& 1 \\
 & add & int	& 1 	& 3 \\
 & bit & int 	& 1 	& 3 \\
 & div & int 	& \texttildelow 21 	& 1 \\
\hline

\end{tabular}
\caption{Operation latency and throughput.}\label{ops}
\end{table}

\subsubsection{Optimising Cache Usage}
Contiguous memory addresses are automatically prefetched the hardware (HW) prefetcher which is triggered on the second cache miss and will monitor accesses within a 4KB-range. HW prefetcher attempts to stay 256 bytes ahead and moniters 8 streams independently at the same time. Excessive HW prefetch saturates memory bus and limits scalability of high throughput parallel programs. OProfile can give averaged memory access bandwidth. The theoretical maximum bandwidth for four memory channels is 51.2GB/s per socket. When memory is saturated, further increase in thread number does not improve performance. \\
Cacheability tools such as stream store and stream read could be applied if the memory addresses in concern are 128-bit aligned, which is not applicable currently. Future optimisation work might involve aligning the arrays in memory. \\



\subsection{Measuring Performance}
At early stage of profiling, OProfile output is used to measure the time spent in each function. However, GCC compiler tends to inline small functions when optimisation is switch on and consequently OProfile wrong detect the function being called. We resolved to using a time difference measurement before and after the function is turned on. Though performance varies between runs, we can still obtain reasonably good estimate with this method. The GNU timer was for this purpose, giving Elapsed Wall time in centi-second precision. \\

Performance varies due to many other factors too, including other tasks running on the same machine, the timing function used and cache access pattern determined by input data types. It is therefore more reasonable to measure statistical performance by average over a large number of repeats, each having realistic workload. Despite that some of the initial measurements still use 1 iterations and relatively small number of memory buffer, in our later tests we processed 100 image frames occupying separate memory locations and repeated this processing 10 times. This test is then repeated another 10 times using the Python script which also computes the mean and standard deviation from these bandwidth measurrments. Measurements are performed under grain size of 3528 element using runtime-determined number of threads. In the final fine-tuning of the library, we varied the number of threads used and the sub-problem size for each thread independently.

\subsection{Results}

Fig \ref{thread} shows the improvement in bandwidth after each modifications. Most significant improvements were seen when parallelisation, compiler optimisation, replacement with bitwise operations, AVX in conjunction with parallel$\_$for were used. These are applicable to other optimisation scenarios also. \\
Optimisations performed lower down the chart are modifications of code higher up on the chart.

% a graph showing how each step gives rise to optimisation
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{fig/barplot.eps}
\caption{Optimisation steps and bandwidth. }

\end{figure}

\subsubsection{Number of Threads}
The number of logical threads are theoretically optimal when it equals the number of physical threads. With HyperThreading switched off, fig .. shows agreement between the number of logical threads used and number of physical threads present. 

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{fig/lineplot_threads.eps}
\caption{Thread-Bandwidth dependence. }\label{thread}

\end{figure}


\subsubsection{Size of Sub-problems}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{fig/lineplot_grain_size.eps}
\caption{Grain size-Bandwidth dependence. The lower two panels show a zoomed plots corresponding to large and small grain sizes.}\label{grain_size}

\end{figure}




\section{Discussion}
As discussed previously, the number of threads used should reflect the number of physical threads and the bandwidth variation with grain size should reflect the memory hierarchy. In fact we observed clear changes in measurements for these two parameters, yet the correspondence is not consistent with our prediction.

The performance is affected by a variety of factors, some tested, and some not within our control. Particularly, the servers we experimented with are based on Non-Uniformed Memory Access architecture, in which access to different part of the RAM takes different amount of time as seen by each processor. For threads accessing data from immediately neightbouring RAM certainly take less time than if the threads are on the other core. However, this effect is hidden by Intel TBB which automatically balances different threads. This is the case even in realistic situation whereby the upstream application writes only to RAM associated with it. \\

HyperThreading and TurboBoost have undertermined influence on the program. Hyperthreading might have negative impact on performance as seen by a comparison between machines with and without HyperThreading switched on. TurboBoost should in theory improve performance by overclocking, yet this has not been tested. 

In addition, the bandwidth measurments heavily rely on the number of frame frames used. Using a high number potentially exercises the cache hierarchy better and improves the bandwidth. 

GNU timer is assummed to be accurate. To have an even more precise and accurate time measurement, the shell timer can be used instead. 

\section{Conclusion}	%page5


%\bibliographystyle{plain}
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{citations}

\begin{thebibliography}{1}

\bibitem{themanual}
Intel, R. \emph{Intel 64 and IA-32 Architectures Optimization Reference Manual.} Intel Corporation, April (2012).

\bibitem{TBBbook}

\bibitem{spec}

\end{thebibliography}


\end{document}


